import streamlit as st
import pandas as pd
import numpy as np
import altair as alt
import geopandas as gpd
import requests
import json # library to handle JSON files
# from streamlit_folium import folium_static
from streamlit_folium import folium_static
import folium # map rendering library
import streamlit_folium as folium_st
from shapely.geometry import Polygon, MultiPolygon
import streamlit.components.v1 as components
import fiona
import jenkspy

def app():
  #Commune
  df_commune = pd.read_csv("./commune_2021.csv", sep=",")
  list_commune = df_commune.loc[:, 'LIBELLE']
  nom_commune = st.sidebar.selectbox(
       "S√©lectionnez votre commune :",
       options=list_commune)
  code_commune = df_commune.loc[df_commune['LIBELLE'] == nom_commune, 'COM'].iloc[0]
  st.sidebar.write('Ma commune:', code_commune, nom_commune)
  #EPCI
  df_epci = pd.read_csv("./EPCI_2020.csv", sep=";")
  nom_epci = df_epci.loc[df_epci['CODGEO'] == code_commune, 'LIBEPCI'].iloc[0]
  code_epci = df_epci.loc[df_epci['CODGEO'] == code_commune, 'EPCI'].iloc[0]
  st.sidebar.write('Mon EPCI:', code_epci, nom_epci)
  #D√©partement
  code_departement = df_commune.loc[df_commune['LIBELLE'] == nom_commune, 'DEP'].iloc[0]
  df_departement = pd.read_csv("./departement2021.csv", dtype={"CHEFLIEU": str}, sep=",")
  nom_departement = df_departement.loc[df_departement['DEP'] == code_departement, 'LIBELLE'].iloc[0]
  st.sidebar.write('Mon d√©partement:', code_departement, nom_departement)
  #R√©gion
  code_region = df_commune.loc[df_commune['LIBELLE'] == nom_commune, 'REG'].iloc[0]
  df_region = pd.read_csv("./region2021.csv", dtype={"CHEFLIEU": str}, sep=",")
  nom_region = df_region.loc[df_region['REG'] == code_region, 'LIBELLE'].iloc[0]
  st.sidebar.write('Ma r√©gion:', str(round(code_region)), nom_region)

  #############################################################################

  st.title('üßë‚Äçüç≥üë∑‚Äç‚ôÇÔ∏èüë©‚Äçüîßüë®‚Äç‚öïÔ∏è ACTIVIT√â ET EMPLOI')
  st.header("Taux de personnes salari√©es en temps partiel ")
  st.caption("Paru le : 19/10/2023 - dernier mill√©sime 2020")
  annee_reference = "2020"
  year = annee_reference[-2:]
  df = pd.read_csv("./activite/iris/base-ic-activite-residents-" + annee_reference + ".csv", dtype={"IRIS":str, "REG": str, "DEP": str, "UU2020": str, "COM": str, "TRIRIS": str, "GRD_QUART": str, "MODIF_IRIS": str, "LAB_IRIS": str}, sep=";")
  df_iris = df.loc[df['COM'] == code_commune]
  df_iris = df_iris[["IRIS", "LIBIRIS", "P" + year + "_SAL15P_TP", "P" + year + "_SAL15P"]]
  df_iris = df_iris.reset_index(drop=True)
  df_iris["tx_temps_partiel"] = ( df_iris["P" + year + "_SAL15P_TP"] / df_iris["P" + year + "_SAL15P"] ) * 100
  st.write(df_iris)

  #################################
  #Comparaison
  #Commune
  df = pd.read_csv("./activite/commune/base-cc-caract_emp-" + annee_reference + ".csv", dtype={"CODGEO":str, "REG": str, "DEP": str}, sep=";")
  df_com = df.loc[df['CODGEO'] == code_commune]
  df_com = df_com[["CODGEO", "LIBGEO", "P" + year + "_SAL15P_TP", "P" + year + "_SAL15P"]]
  df_com = df_com.reset_index(drop=True)
  tpartiel_com = ( df_com["P" + year +"_SAL15P_TP"].iloc[0] / df_com["P" + year +"_SAL15P"].iloc[0] ) * 100
  #EPCI
  epci_select = pd.read_csv('./EPCI_2020.csv', dtype={"CODGEO": str, "DEP": str, "REG": str, "EPCI":str}, sep = ';')
  df_epci = pd.merge(df[['CODGEO', "P" + year + "_SAL15P_TP", "P" + year + "_SAL15P" ]], epci_select[['CODGEO','EPCI', 'LIBEPCI']], left_on='CODGEO', right_on='CODGEO')
  df_epci = df_epci.loc[df_epci["EPCI"] == str(code_epci), ['EPCI', 'LIBEPCI', 'CODGEO',"P" + year + "_SAL15P_TP", "P" + year + "_SAL15P"]]
  df_epci = df_epci.reset_index(drop=True)
  tpartiel_epci = (df_epci.loc[:, "P" + year +"_SAL15P_TP"].sum() / df_epci["P" + year +"_SAL15P"].sum()) * 100
  #DEP
  df_dep = df.loc[df['DEP'] == code_departement]
  df_dep = df_dep[["P" + year + "_SAL15P_TP", "P" + year + "_SAL15P"]]
  df_dep = df_dep.reset_index(drop=True)
  tpartiel_dep = (df_dep.loc[:, "P" + year +"_SAL15P_TP"].sum() / df_dep["P" + year +"_SAL15P"].sum()) * 100
  #REG
  df_reg = df.loc[df['REG'] == str(round(code_region))]
  df_reg = df_reg[["P" + year + "_SAL15P_TP", "P" + year + "_SAL15P"]]
  df_reg = df_reg.reset_index(drop=True)
  tpartiel_reg = (df_reg.loc[:, "P" + year +"_SAL15P_TP"].sum() / df_reg["P" + year +"_SAL15P"].sum()) * 100
  #France
  tpartiel_fr = (df.loc[:, "P" + year +"_SAL15P_TP"].sum() / df["P" + year +"_SAL15P"].sum()) * 100
  #Comparaison
  d = {'Territoires': [nom_commune, nom_epci, nom_departement, nom_region, 'France'], "Part du temps partiel de 15 √† 64 ans - " + annee_reference + " (en %)": [tpartiel_com, tpartiel_epci, tpartiel_dep, tpartiel_reg, tpartiel_fr]}
  df_global_tp = pd.DataFrame(data=d)
  st.write(df_global_tp)
  #####################
  # Carte Temps partiel
  # URL de l'API
  url = "https://public.opendatasoft.com/api/records/1.0/search/?dataset=georef-france-iris-millesime&q=&rows=500&sort=year&facet=year&facet=reg_name&facet=dep_name&facet=arrdep_name&facet=ze2020_name&facet=bv2012_name&facet=epci_name&facet=ept_name&facet=com_name&facet=com_arm_name&facet=iris_name&facet=iris_area_code&facet=iris_type&facet=com_code&refine.year=2022&refine.com_code=" + code_commune
  # Appel √† l'API
  response = requests.get(url)
  # Conversion de la r√©ponse en JSON
  data = response.json()
  # Normalisation des donn√©es pour obtenir un DataFrame pandas
  df = pd.json_normalize(data['records'])
  # S√©paration des latitudes et longitudes
  latitudes, longitudes = zip(*df['fields.geo_point_2d'])
  # Conversion du DataFrame en GeoDataFrame
  gdf = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(longitudes, latitudes))
  # Supposons que 'gdf' est votre GeoDataFrame original
  gdf.set_crs(epsg=4326, inplace=True)  # D√©finir le syst√®me de coordonn√©es actuel si ce n'est pas d√©j√† fait

  def to_multipolygon(coords):
      def is_nested_list(lst):
          return any(isinstance(i, list) for i in lst)

      if len(coords) == 0:
          return None

      # If the first element of coords is a list of lists, we're dealing with a MultiPolygon
      if is_nested_list(coords[0]):
          polygons = []
          for poly_coords in coords:
              # If the first element of poly_coords is a list of lists, we're dealing with a Polygon with holes
              if is_nested_list(poly_coords[0]):
                  polygons.append(Polygon(shell=poly_coords[0], holes=poly_coords[1:]))
              else:
                  polygons.append(Polygon(poly_coords))
          return MultiPolygon(polygons)
      else:
          return Polygon(coords)


  # Convertir les coordonn√©es des fronti√®res en objets Polygon ou MultiPolygon
  gdf['geometry'] = gdf['fields.geo_shape.coordinates'].apply(to_multipolygon)

  # Joindre le dataframe de population avec le GeoDataFrame
  gdf = gdf.merge(df_iris, left_on='fields.iris_code', right_on="IRIS")

  # Cr√©er une carte centr√©e autour de la latitude et longitude moyenne
  map_center = [gdf['geometry'].centroid.y.mean(), gdf['geometry'].centroid.x.mean()]
  # Calcul des intervals avec la m√©thode de Jenks
  # Supprimer les lignes avec NaN pour le calcul de la m√©thode de Jenks
  gdf = gdf.dropna(subset=['tx_temps_partiel'])
  # S'assurer que toutes les valeurs sont finies
  gdf = gdf[pd.to_numeric(gdf['tx_temps_partiel'], errors='coerce').notnull()]
  breaks = jenkspy.jenks_breaks(gdf['tx_temps_partiel'], 5)
  m = folium.Map(location=map_center, zoom_start=12, control_scale=True, tiles='cartodb positron', attr='SCOP COPAS')

  # Ajouter la carte choropl√®the
  folium.Choropleth(
    geo_data=gdf.set_index("IRIS"),
    name='choropleth',
    data=gdf,
    columns=["IRIS", "tx_temps_partiel"],
    key_on='feature.id',
    fill_color='YlOrRd',
    fill_opacity=0.7,
    line_opacity=0.2,
    color='#ffffff',
    weight=3,
    opacity=1.0,
    legend_name='Taux de temps partiel',
    bins=breaks
  ).add_to(m)

  folium.LayerControl().add_to(m)

  style_function = lambda x: {'fillColor': '#ffffff',
                          'color':'#000000',
                          'fillOpacity': 0.1,
                          'weight': 0.1}
  highlight_function = lambda x: {'fillColor': '#000000',
                                'color':'#000000',
                                'fillOpacity': 0.50,
                                'weight': 0.1}
  NIL = folium.features.GeoJson(
    gdf,
    style_function=style_function,
    control=False,
    highlight_function=highlight_function,
    tooltip=folium.features.GeoJsonTooltip(
        fields=["LIBIRIS", "tx_temps_partiel"],
        aliases=['Iris: ', "Taux de temps partiel :"],
        style=("background-color: white; color: #333333; font-family: arial; font-size: 12px; padding: 10px;")
    )
  )
  m.add_child(NIL)
  m.keep_in_front(NIL)
  st.subheader("Taux de temps partiel par IRIS")
  # Afficher la carte dans Streamlit
  folium_st.folium_static(m)

  ###########################################################################
  st.header("Taux de chomage des 15-64 ans")
  st.caption("Paru le : 19/10/2023 - dernier mill√©sime 2020")
  last_year = "2020"
  year = last_year[-2:]
  df = pd.read_csv("./activite/iris/base-ic-activite-residents-" + last_year + ".csv", dtype={"IRIS":str, "REG": str, "DEP": str, "UU2020": str, "COM": str, "TRIRIS": str, "GRD_QUART": str, "MODIF_IRIS": str, "LAB_IRIS": str}, sep=";")
  df_iris = df.loc[df['COM'] == code_commune]
  df_iris = df_iris[["IRIS", "LIBIRIS", "P" + year + "_CHOM1564", "P" + year +"_ACT1564"]]
  df_iris = df_iris.reset_index(drop=True)
  df_iris["tx_chomage"] = ( df_iris["P" + year + "_CHOM1564"] / df_iris["P" + year + "_ACT1564"] ) * 100
  st.write(df_iris)
  #############################

  # Comparaison
  #Commune
  df = pd.read_csv("./activite/commune/base-cc-emploi-pop-active-" + last_year + ".csv", dtype={"CODGEO":str, "REG": str, "DEP": str}, sep=";")
  df_com = df.loc[df['CODGEO'] == code_commune]
  df_com = df_com[["CODGEO", "LIBGEO", "P" + year + "_CHOM1564", "P" + year + "_ACT1564"]]
  df_com = df_com.reset_index(drop=True)
  chom_com = (df_com.loc[:, "P" + year +"_CHOM1564"].iloc[0] / df_com["P" + year +"_ACT1564"].iloc[0]) * 100
  #EPCI
  epci_select = pd.read_csv('./EPCI_2020.csv', dtype={"CODGEO": str, "DEP": str, "REG": str, "EPCI":str}, sep = ';')
  df_epci = pd.merge(df[['CODGEO', "P" + year + "_CHOM1564", "P" + year + "_ACT1564" ]], epci_select[['CODGEO','EPCI', 'LIBEPCI']], left_on='CODGEO', right_on='CODGEO')
  df_epci = df_epci.loc[df_epci["EPCI"] == str(code_epci), ['EPCI', 'LIBEPCI', 'CODGEO',"P" + year + "_CHOM1564", "P" + year + "_ACT1564"]]
  df_epci = df_epci.reset_index(drop=True)
  chom_epci = (df_epci.loc[:, "P" + year +"_CHOM1564"].sum() / df_epci["P" + year +"_ACT1564"].sum()) * 100
  #DEP
  df_dep = df.loc[df['DEP'] == code_departement]
  df_dep = df_dep[["P" + year + "_CHOM1564", "P" + year + "_ACT1564"]]
  df_dep = df_dep.reset_index(drop=True)
  chom_dep = (df_dep.loc[:, "P" + year +"_CHOM1564"].sum() / df_dep["P" + year +"_ACT1564"].sum()) * 100
  #REG
  df_reg = df.loc[df['REG'] == str(round(code_region))]
  df_reg = df_reg[["P" + year + "_CHOM1564", "P" + year + "_ACT1564"]]
  df_reg = df_reg.reset_index(drop=True)
  chom_reg = (df_reg.loc[:, "P" + year +"_CHOM1564"].sum() / df_reg["P" + year +"_ACT1564"].sum()) * 100
  #France
  chom_fr = (df.loc[:, "P" + year +"_CHOM1564"].sum() / df["P" + year +"_ACT1564"].sum()) * 100
  #Comparaison
  d = {'Territoires': [nom_commune, nom_epci, nom_departement, nom_region, 'France'], "Part des ch√¥meurs de 15 √† 64 ans - " + annee_reference + " (en %)": [chom_com, chom_epci, chom_dep, chom_reg, chom_fr]}
  df = pd.DataFrame(data=d)
  st.write(df)
  ########################

  # Carte Taux de comage
  # URL de l'API
  url = "https://public.opendatasoft.com/api/records/1.0/search/?dataset=georef-france-iris-millesime&q=&rows=500&sort=year&facet=year&facet=reg_name&facet=dep_name&facet=arrdep_name&facet=ze2020_name&facet=bv2012_name&facet=epci_name&facet=ept_name&facet=com_name&facet=com_arm_name&facet=iris_name&facet=iris_area_code&facet=iris_type&facet=com_code&refine.year=2022&refine.com_code=" + code_commune
  # Appel √† l'API
  response = requests.get(url)
  # Conversion de la r√©ponse en JSON
  data = response.json()
  # Normalisation des donn√©es pour obtenir un DataFrame pandas
  df = pd.json_normalize(data['records'])
  # S√©paration des latitudes et longitudes
  latitudes, longitudes = zip(*df['fields.geo_point_2d'])
  # Conversion du DataFrame en GeoDataFrame
  gdf = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(longitudes, latitudes))
  # Supposons que 'gdf' est votre GeoDataFrame original
  gdf.set_crs(epsg=4326, inplace=True)  # D√©finir le syst√®me de coordonn√©es actuel si ce n'est pas d√©j√† fait

  def to_multipolygon(coords):
      def is_nested_list(lst):
          return any(isinstance(i, list) for i in lst)

      if len(coords) == 0:
          return None

      # If the first element of coords is a list of lists, we're dealing with a MultiPolygon
      if is_nested_list(coords[0]):
          polygons = []
          for poly_coords in coords:
              # If the first element of poly_coords is a list of lists, we're dealing with a Polygon with holes
              if is_nested_list(poly_coords[0]):
                  polygons.append(Polygon(shell=poly_coords[0], holes=poly_coords[1:]))
              else:
                  polygons.append(Polygon(poly_coords))
          return MultiPolygon(polygons)
      else:
          return Polygon(coords)


  # Convertir les coordonn√©es des fronti√®res en objets Polygon ou MultiPolygon
  gdf['geometry'] = gdf['fields.geo_shape.coordinates'].apply(to_multipolygon)

  # Joindre le dataframe de population avec le GeoDataFrame
  gdf = gdf.merge(df_iris, left_on='fields.iris_code', right_on="IRIS")

  # Cr√©er une carte centr√©e autour de la latitude et longitude moyenne
  map_center = [gdf['geometry'].centroid.y.mean(), gdf['geometry'].centroid.x.mean()]
  # Supprimer les lignes avec NaN pour le calcul de la m√©thode de Jenks
  gdf = gdf.dropna(subset=['tx_chomage'])
  # S'assurer que toutes les valeurs sont finies
  gdf = gdf[pd.to_numeric(gdf['tx_chomage'], errors='coerce').notnull()]
  breaks = jenkspy.jenks_breaks(gdf['tx_chomage'], 5)
  m = folium.Map(location=map_center, zoom_start=12, control_scale=True, tiles='cartodb positron', attr='SCOP COPAS')

  # Ajouter la carte choropl√®the
  folium.Choropleth(
    geo_data=gdf.set_index("IRIS"),
    name='choropleth',
    data=gdf,
    columns=["IRIS", "tx_chomage"],
    key_on='feature.id',
    fill_color='YlOrRd',
    fill_opacity=0.7,
    line_opacity=0.2,
    color='#ffffff',
    weight=3,
    opacity=1.0,
    legend_name='Part des ch√¥meurs de 15-64 ans',
    bins=breaks
  ).add_to(m)

  folium.LayerControl().add_to(m)

  style_function = lambda x: {'fillColor': '#ffffff',
                          'color':'#000000',
                          'fillOpacity': 0.1,
                          'weight': 0.1}
  highlight_function = lambda x: {'fillColor': '#000000',
                                'color':'#000000',
                                'fillOpacity': 0.50,
                                'weight': 0.1}
  NIL = folium.features.GeoJson(
    gdf,
    style_function=style_function,
    control=False,
    highlight_function=highlight_function,
    tooltip=folium.features.GeoJsonTooltip(
        fields=["LIBIRIS", "tx_chomage"],
        aliases=['Iris: ', "Taux de ch√¥mage :"],
        style=("background-color: white; color: #333333; font-family: arial; font-size: 12px; padding: 10px;")
    )
  )
  m.add_child(NIL)
  m.keep_in_front(NIL)
  st.subheader("Part des ch√¥meurs de 15-64 ans par IRIS")
  # Afficher la carte dans Streamlit
  folium_st.folium_static(m)
  ###########################################################################

  st.header("Taux d'emploi des 15-64 ans")
  st.subheader("Zoom QPV")
  last_year_qpv = "2022"
  def tx_emploi_qpv(fichier, nom_ville, annee) :
    fp_qpv = "./qpv.geojson"
    map_qpv_df = gpd.read_file(fp_qpv)
    year = annee[-2:]
    df = pd.read_csv(fichier, dtype={"CODGEO": str}, sep=";")
    map_qpv_df_code_insee = map_qpv_df.merge(df, left_on='code_qp', right_on='CODGEO')
    map_qpv_df_code_insee_extract = map_qpv_df_code_insee[['nom_qp', 'code_qp', 'commune_qp','code_insee', 'TX_TOT_EMPL' ]]
    map_qpv_df_code_insee_extract
    df_qpv = map_qpv_df_code_insee_extract.loc[map_qpv_df_code_insee_extract["commune_qp"].str.contains(nom_ville + "(,|$)")]
    df_qpv = df_qpv.reset_index(drop=True)
    df_qpv = df_qpv[['code_qp', 'nom_qp','commune_qp', 'TX_TOT_EMPL']]
    df_qpv = df_qpv.rename(columns={'nom_qp': "Nom du quartier",'code_qp' : "Code du quartier", "commune_qp" : "Communes concern√©es", 'TX_TOT_EMPL' : "Taux d'emploi des 15/65 ans " + last_year_qpv})
    return df_qpv

  tx_emploi_qpv = tx_emploi_qpv('./activite/insertion-pro-qpv/IPRO_' + last_year_qpv + '.csv', nom_commune, last_year_qpv)
  st.table(tx_emploi_qpv)

  st.header("Part des emplois √† dur√©e limit√©e parmi les emplois (ou emplois pr√©caires)")
  st.subheader("Zoom QPV")

  def tx_emploi_limit_qpv(fichier, nom_ville, annee) :
    fp_qpv = "./qpv.geojson"
    map_qpv_df = gpd.read_file(fp_qpv)
    year = annee[-2:]
    df = pd.read_csv(fichier, dtype={"CODGEO": str}, sep=";")
    map_qpv_df_code_insee = map_qpv_df.merge(df, left_on='code_qp', right_on='CODGEO')
    if int(annee) >= 2021:
      map_qpv_df_code_insee_extract = map_qpv_df_code_insee[['nom_qp', 'code_qp', 'commune_qp','code_insee', 'TX_TOT_EDLIM' ]]
      map_qpv_df_code_insee_extract
      df_qpv = map_qpv_df_code_insee_extract.loc[map_qpv_df_code_insee_extract["commune_qp"].str.contains(nom_ville + "(,|$)")]
      df_qpv = df_qpv.reset_index(drop=True)
      df_qpv = df_qpv[['code_qp', 'nom_qp','commune_qp', 'TX_TOT_EDLIM']]
      df_qpv = df_qpv.rename(columns={'nom_qp': "Nom du quartier",'code_qp' : "Code du quartier", "commune_qp" : "Communes concern√©es", 'TX_TOT_EDLIM' : "Part des emplois √† dur√©e limit√©e " + last_year_qpv})
    else:
      map_qpv_df_code_insee_extract = map_qpv_df_code_insee[['nom_qp', 'code_qp', 'commune_qp','code_insee', 'TX_TOT_EPREC' ]]
      map_qpv_df_code_insee_extract
      df_qpv = map_qpv_df_code_insee_extract.loc[map_qpv_df_code_insee_extract["commune_qp"].str.contains(nom_ville + "(,|$)")]
      df_qpv = df_qpv.reset_index(drop=True)
      df_qpv = df_qpv[['code_qp', 'nom_qp','commune_qp', 'TX_TOT_EPREC']]
      df_qpv = df_qpv.rename(columns={'nom_qp': "Nom du quartier",'code_qp' : "Code du quartier", "commune_qp" : "Communes concern√©es", 'TX_TOT_EPREC' : "Part des emplois √† dur√©e limit√©e " + last_year_qpv})
    return df_qpv

  tx_emploi_limit_qpv = tx_emploi_limit_qpv('./activite/insertion-pro-qpv/IPRO_' + last_year_qpv + '.csv', nom_commune, last_year_qpv)
  st.table(tx_emploi_limit_qpv)

  st.caption("Emploi √† dur√©e limit√©e : contrat d'apprentissage, Plac√©s par une agence d'int√©rim, Emplois-jeunes, CES, contrats de qualification, stagiaires r√©mun√©r√©s en entreprise, autres emplois √† dur√©e limit√©e")
